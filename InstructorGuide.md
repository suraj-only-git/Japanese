# 目次

- 概要

- ラボ資格情報

- データフロー テンプレートをインポートする	

    データフロー テンプレートをインポートする方法	

- デモの要件	

- 環境

    ラボ環境を構成する方法	

    予測のためのノートブックを作成する方法	


# 概要

このドキュメントでは、次の内容についてガイドラインを提供します。

• ラボ資格情報

• データフロー テンプレートをインポートする方法

• Spark ライブラリを読み込んで環境を構成する方法

• ノートブックを作成する方法

• データ サイエンス モデルを使用して予測を作成する方法

• 出力をセマンティック モデルに保存する方法


**免責事項**: 製品は日々変更されているため、一部のスクリーンショットが最新でない場合があることに注意してください。次回更新時に修正できるよう取り組んでまいります。


# ラボ資格情報:

出席者のいずれかが別の環境でラボを完了することを選択した場合、共有する必要がある資格情報を次に示します。

出席者は、Dataverse および SharePoint に接続するためにラボアカウントに関連付けられたユーザー名とパスワードが必要になります。

Snowflake ユーザー名: **TE_SNOWFLAKE**

Snowflake パスワード: **8UpfRpExVDXv2AC**

ADLS Gen2 アカウントキー: **Lpwn8hQASMpe5r4F+VFXAvpnzKF9x9Kjt5GMvMCFWB0xCFuM4fyVwOW6rF200bTop3LpKpsIno/T+AStx6cz6w==**
 

# データフロー テンプレートをインポートする:

講師は、出席者にデータフロー テンプレートをインポートさせるかどうかを選択できます。テンプレートをインポートするステップは次のとおりです。

## データフロー テンプレートをインポートする方法

1. **ラボ 2、タスク 8 で作成し、FAIAD_<ユーザー名> という名前を付けた Fabric ワークスペース**に移動します。FAIAD_demouser という名前を付けました。

2. **Data Factory ホーム** ページに移動します。

3. メニューから、**新規 -> データフロー (Gen2)** を選択します。
 
4. Power Query ウィンドウが開きます。中央のペインで、**Power Query テンプレートからインポートする**を選択します。
 
5. **デスクトップ -> ソリューション** フォルダーを参照し、インポートするデータフローを選択します。ここでは、**df_People_SharePoint.pqt** をインポートします。

6. **開く**を選択します。

インポートされると、クエリとそのクエリのすべてのステップがインポートされることに注目してください。ただし、接続を構成する必要があります。また、データ送信先も設定する必要があります。ラボの指示に従って、これらのステップを完了してください。

 

# デモの要件

次のステップに進む前に、講師はラボ 1 - 6 を完了し、すべてのデータを取り込む必要があります。

# 環境

## ラボ環境を構成する方法

**注**: ライブラリのインストールには時間がかかるため、デモの前にラボ環境を構成することをお勧めします。出席者に次の手順を説明できます。


7. **ラボ 2、タスク 8 で作成し、FAIAD_<ユーザー名> という名前を付けた Fabric ワークスペース**に移動します。

8. 上部のメニューで、**省略記号 (…)** を選択します。

9. **ワークスペースの設定**を選択します。
 
10. [ワークスペースの設定] ダイアログが開きます。左側のメニューで、**データ エンジニアリング/サイエンス**を展開します。

11. **Spark の設定**を選択します。

12. [Spark の設定] メニューで、**環境**タブを選択します。

13. **既定の環境を設定する**スライダーを**オン**に変更します。

14. **ワークスペースの既定値**ドロップダウンを選択します。

15. **新しい環境**を選択します。
 
16. [新しい環境] ダイアログが開きます。名前を **FAIAD_<ユーザー名>_env** と入力します。


**注**: ワークスペース名は一意である必要があります。このドキュメントではワークスペース名として FAIAD_demouser_env を使用しています。ただし、自分のワークスペース名は異なるものにする必要があります。[名前] フィールドの下に、**この名前は使用できます**という緑色のチェック マークが表示されていることを確認してください。


17. **作成**を選択します。
 
18. パブリック ライブラリとカスタム ライブラリを追加する画面が表示されます。パブリック ライブラリである Prophet を追加します。上部のメニューで、**パブリック ライブラリ -> PyPI から追加**を選択します。

19. 中央のペインで、[ライブラリ] の下のテキストボックスに **prophet** と入力します。

    **注: バージョンが 1.1.5** であることを確認します。

20. 右側のペインで、**公開**をクリックします。
 
21. [保留中の変更] ダイアログが開きます。**すべて公開**を選択します。
22. [すべての変更を発行しますか?] ダイアログが開きます。**公開**を選択します。更新の公開には数分かかります。
 
23. **進捗を見る**を選択して進行状況を確認します。更新の公開には数分かかります。
 
24. インストールされると、**状態が成功**に変わります。
 
25. これで環境が構成されました。これをワークスペースの既定の環境として保存する必要があります。左側のパネルから **FAIAD_<ユーザー名>** を選択します。

26. 上部のメニューから、**ワークスペースの設定** (または [省略記号] -> [ワークスペースの設定]) を選択します。
 
27. [ワークスペースの設定] ダイアログが開きます。左側のメニューで、**データ エンジニアリング/サイエンス**を展開します。

28. **Spark の設定**を選択します。

29. [Spark の設定] メニューで、**環境**タブを選択します。

30. **既定の環境**のスライダーを**オン**に設定します。

31. **ワークスペースの既定値**ドロップダウンを選択します。

32. 作成したばかりの環境 **(FAIAD_<ユーザー名>_env)** をドロップダウンで選択します。

33. **保存**を選択します。
 

## 予測のためのノートブックを作成する方法

34. **Synapse Data Engineering ホーム** ページに移動します。

35. **新規 -> ノートブック**を選択します。
 
36. ノートブック、言語、環境、新しいセルの作成方法など、レイアウトについて**概要を簡単に**説明します。

37. **新しいセル**を作成します。

38. 次の**コード**を入力します。

```
from pyspark.sql import SparkSession

from pyspark.sql.functions import month, year, col

from prophet import Prophet

import pandas as pd

# Initialize Spark session
spark = SparkSession.builder.appName("Prophet Forecasting").getOrCreate()

# Load data from your specific Spark table
df = spark.sql("SELECT * FROM lh_FAIAD.Sales")

# Aggregate data to monthly level
monthly_df = df.withColumn("Month", month("InvoiceDate"))\
               .withColumn("Year", year("InvoiceDate"))\
               .groupBy("Year", "Month")\
               .sum("Quantity")\
               .orderBy("Year", "Month")

# Convert to Pandas DataFrame and prepare for Prophet
pandas_df = monthly_df.toPandas()
pandas_df['ds'] = pd.to_datetime(pandas_df[['Year', 'Month']].assign(DAY=1))
pandas_df['y'] = pandas_df['sum(Quantity)']

# Fit the Prophet model
model = Prophet(yearly_seasonality=True, weekly_seasonality=False,daily_seasonality=False)
model.fit(pandas_df[['ds, 'y']])

# Create a DataFrame for future predictions (e.g., next 12 months)
future = model.make_future_dataframe(periods=12, freq='M')

# Forecast
forecast = model.predict(future)

# Plotting the forecast
model.plot(forecast)
model.plot_components(forecast)
```


39. **コード**の各ステップについて説明します (ヒントはコメントとして提供されています)。

40. セルの横にある**再生**ボタンを選択してコードを実行します。

 
作成される 3 つのグラフ (後述) について出席者に説明します。実績は 2023 年 4 月まであり、12 か月間について予測しています。


**最初のグラフ**では、2024 年 4 月まで季節性と予測が削除されています。


**2 つ目のグラフ**では、2024 年 4 月まで、傾向が削除され季節性が追加されています。

 
**3 つ目のグラフ**は、傾向と季節性の両方を使用して予測しています。このグラフには上限と下限も示されています。

41. **新しいセル**を作成します。

42.  次の**コード**をセルに追加します。


```
display(forecast)
#write forecast data to a table
spark.createDataFrame(forecast).write.saveAsTable("Sales_Forecast", mode="overwrite")
```

43. **再生**ボタンを選択してセルを実行します。
 
44. **表示されるデータ**について出席者に説明します。

45. 新しいテーブル **sales_forecast** が作成されたことをユーザーに示します。
 
46. このテーブルを**クエリ**して、テーブルの内容をユーザーに示します。

